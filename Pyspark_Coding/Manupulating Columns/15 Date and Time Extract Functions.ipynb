{"cells":[{"cell_type":"markdown","source":["## Date and Time Extract Functions\nLet us get an overview about Date and Time extract functions. Here are the extract functions that are useful which are self explanatory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6748b0e2-6ced-472d-9808-7791974d3972"}}},{"cell_type":"markdown","source":["* `year`\n* `month`\n* `weekofyear`\n* `dayofyear`\n* `dayofmonth`\n* `dayofweek`\n* `hour`\n* `minute`\n* `second`\n\nThere might be few more functions. You can review based up on your requirements."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5e89dac-50bc-41e8-996c-ea7e82b9da44"}}},{"cell_type":"code","source":["l = [(\"X\", )]"],"metadata":{"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e4bfa6d-1bc4-4594-9719-ecf1034323a8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.createDataFrame(l).toDF(\"dummy\")"],"metadata":{"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dfb2b51e-ccb0-45ef-aecd-ee2011f3375c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbad2822-d679-4516-9c1a-45cef06dd9ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----+\n|dummy|\n+-----+\n|    X|\n+-----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+\ndummy|\n+-----+\n    X|\n+-----+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import year, month, weekofyear, dayofmonth, \\\n    dayofyear, dayofweek, current_date"],"metadata":{"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bd0f360-5434-4888-a802-048e1a0d14bc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.select(\n    current_date().alias('current_date'), \n    year(current_date()).alias('year'),\n    month(current_date()).alias('month'),\n    weekofyear(current_date()).alias('weekofyear'),\n    dayofyear(current_date()).alias('dayofyear'),\n    dayofmonth(current_date()).alias('dayofmonth'),\n    dayofweek(current_date()).alias('dayofweek')\n).show() #yyyy-MM-dd"],"metadata":{"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"296203c0-2415-40e6-94ce-2863af158a3d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+----+-----+----------+---------+----------+---------+\n|current_date|year|month|weekofyear|dayofyear|dayofmonth|dayofweek|\n+------------+----+-----+----------+---------+----------+---------+\n|  2022-01-02|2022|    1|        52|        2|         2|        1|\n+------------+----+-----+----------+---------+----------+---------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+----+-----+----------+---------+----------+---------+\ncurrent_date|year|month|weekofyear|dayofyear|dayofmonth|dayofweek|\n+------------+----+-----+----------+---------+----------+---------+\n  2022-01-02|2022|    1|        52|        2|         2|        1|\n+------------+----+-----+----------+---------+----------+---------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["help(dayofweek)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2d86e36-d0e2-446f-82bf-fb17ac53e738"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on function dayofweek in module pyspark.sql.functions:\n\ndayofweek(col)\n    Extract the day of the week of a given date as integer.\n    \n    .. versionadded:: 2.3.0\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])\n    &gt;&gt;&gt; df.select(dayofweek(&#39;dt&#39;).alias(&#39;day&#39;)).collect()\n    [Row(day=4)]\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on function dayofweek in module pyspark.sql.functions:\n\ndayofweek(col)\n    Extract the day of the week of a given date as integer.\n    \n    .. versionadded:: 2.3.0\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])\n    &gt;&gt;&gt; df.select(dayofweek(&#39;dt&#39;).alias(&#39;day&#39;)).collect()\n    [Row(day=4)]\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import current_timestamp, hour, minute, second"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35ed882c-f24d-462a-aa23-5275f1b7a60d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94eb58e0-cb8e-48b6-a4c9-629830883bdd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["help(functions)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc319e2b-f75f-49e6-b603-16670070ea1b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on module pyspark.sql.functions in pyspark.sql:\n\nNAME\n    pyspark.sql.functions - A collections of builtin functions\n\nFUNCTIONS\n    abs(col)\n        Computes the absolute value.\n        \n        .. versionadded:: 1.3\n    \n    acos(col)\n        .. versionadded:: 1.4.0\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            inverse cosine of `col`, as if computed by `java.lang.Math.acos()`\n    \n    acosh(col)\n        Computes inverse hyperbolic cosine of the input column.\n        \n        .. versionadded:: 3.1.0\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n    \n    add_months(start, months)\n        Returns the date that is `months` months after `start`\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])\n        &gt;&gt;&gt; df.select(add_months(df.dt, 1).alias(&#39;next_month&#39;)).collect()\n        [Row(next_month=datetime.date(2015, 5, 8))]\n    \n    aggregate(col, initialValue, merge, finish=None)\n        Applies a binary operator to an initial state and all elements in the array,\n        and reduces this to a single state. The final state is converted into the final result\n        by applying a finish function.\n        \n        Both functions can use methods of :class:`~pyspark.sql.Column`, functions defined in\n        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n        Python ``UserDefinedFunctions`` are not supported\n        (`SPARK-27052 &lt;https://issues.apache.org/jira/browse/SPARK-27052&gt;`__).\n        \n        .. versionadded:: 3.1.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        initialValue : :class:`~pyspark.sql.Column` or str\n            initial value. Name of column or expression\n        merge : function\n            a binary function ``(acc: Column, x: Column) -&gt; Column...`` returning expression\n            of the same type as ``zero``\n        finish : function\n            an optional unary function ``(x: Column) -&gt; Column: ...``\n            used to convert accumulated value.\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (&#34;id&#34;, &#34;values&#34;))\n        &gt;&gt;&gt; df.select(aggregate(&#34;values&#34;, lit(0.0), lambda acc, x: acc + x).alias(&#34;sum&#34;)).show()\n        +----+\n        | sum|\n        +----+\n        |42.0|\n        +----+\n        \n        &gt;&gt;&gt; def merge(acc, x):\n        ...     count = acc.count + 1\n        ...     sum = acc.sum + x\n        ...     return struct(count.alias(&#34;count&#34;), sum.alias(&#34;sum&#34;))\n        &gt;&gt;&gt; df.select(\n        ...     aggregate(\n        ...         &#34;values&#34;,\n        ...         struct(lit(0).alias(&#34;count&#34;), lit(0.0).alias(&#34;sum&#34;)),\n        ...         merge,\n        ...         lambda acc: acc.sum / acc.count,\n        ...     ).alias(&#34;mean&#34;)\n        ... ).show()\n        +----+\n        |mean|\n        +----+\n        | 8.4|\n        +----+\n    \n    approxCountDistinct(col, rsd=None)\n        .. deprecated:: 2.1.0\n            Use :func:`approx_count_distinct` instead.\n        \n        .. versionadded:: 1.3\n    \n    approx_count_distinct(col, rsd=None)\n        Aggregate function: returns a new :class:`~pyspark.sql.Column` for approximate distinct count\n        of column `col`.\n        \n        .. versionadded:: 2.1.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n        rsd : float, optional\n            maximum relative standard deviation allowed (default = 0.05).\n            For rsd &lt; 0.01, it is more efficient to use :func:`countDistinct`\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df.agg(approx_count_distinct(df.age).alias(&#39;distinct_ages&#39;)).collect()\n        [Row(distinct_ages=2)]\n    \n    array(*cols)\n        Creates a new array column.\n        \n        .. versionadded:: 1.4.0\n        \n        Parameters\n        ----------\n        cols : :class:`~pyspark.sql.Column` or str\n            column names or :class:`~pyspark.sql.Column`\\s that have\n            the same data type.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df.select(array(&#39;age&#39;, &#39;age&#39;).alias(&#34;arr&#34;)).collect()\n        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n        &gt;&gt;&gt; df.select(array([df.age, df.age]).alias(&#34;arr&#34;)).collect()\n        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n    \n    array_contains(col, value)\n        Collection function: returns null if the array is null, true if the array contains the\n        given value, and false otherwise.\n        \n        .. versionadded:: 1.5.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        value :\n            value or column to check for in array\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([&#34;a&#34;, &#34;b&#34;, &#34;c&#34;],), ([],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_contains(df.data, &#34;a&#34;)).collect()\n        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n        &gt;&gt;&gt; df.select(array_contains(df.data, lit(&#34;a&#34;))).collect()\n        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n    \n    array_distinct(col)\n        Collection function: removes duplicate values from the array.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_distinct(df.data)).collect()\n        [Row(array_distinct(data)=[1, 2, 3]), Row(array_distinct(data)=[4, 5])]\n    \n    array_except(col1, col2)\n        Collection function: returns an array of the elements in col1 but not in col2,\n        without duplicates.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col1 : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        col2 : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql import Row\n        &gt;&gt;&gt; df = spark.createDataFrame([Row(c1=[&#34;b&#34;, &#34;a&#34;, &#34;c&#34;], c2=[&#34;c&#34;, &#34;d&#34;, &#34;a&#34;, &#34;f&#34;])])\n        &gt;&gt;&gt; df.select(array_except(df.c1, df.c2)).collect()\n        [Row(array_except(c1, c2)=[&#39;b&#39;])]\n    \n    array_intersect(col1, col2)\n        Collection function: returns an array of the elements in the intersection of col1 and col2,\n        without duplicates.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col1 : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        col2 : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql import Row\n        &gt;&gt;&gt; df = spark.createDataFrame([Row(c1=[&#34;b&#34;, &#34;a&#34;, &#34;c&#34;], c2=[&#34;c&#34;, &#34;d&#34;, &#34;a&#34;, &#34;f&#34;])])\n        &gt;&gt;&gt; df.select(array_intersect(df.c1, df.c2)).collect()\n        [Row(array_intersect(c1, c2)=[&#39;a&#39;, &#39;c&#39;])]\n    \n    array_join(col, delimiter, null_replacement=None)\n        Concatenates the elements of `column` using the `delimiter`. Null values are replaced with\n        `null_replacement` if set, otherwise they are ignored.\n        \n        .. versionadded:: 2.4.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([&#34;a&#34;, &#34;b&#34;, &#34;c&#34;],), ([&#34;a&#34;, None],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_join(df.data, &#34;,&#34;).alias(&#34;joined&#34;)).collect()\n        [Row(joined=&#39;a,b,c&#39;), Row(joined=&#39;a&#39;)]\n        &gt;&gt;&gt; df.select(array_join(df.data, &#34;,&#34;, &#34;NULL&#34;).alias(&#34;joined&#34;)).collect()\n        [Row(joined=&#39;a,b,c&#39;), Row(joined=&#39;a,NULL&#39;)]\n    \n    array_max(col)\n        Collection function: returns the maximum value of the array.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_max(df.data).alias(&#39;max&#39;)).collect()\n        [Row(max=3), Row(max=10)]\n    \n    array_min(col)\n        Collection function: returns the minimum value of the array.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_min(df.data).alias(&#39;min&#39;)).collect()\n        [Row(min=1), Row(min=-1)]\n    \n    array_position(col, value)\n        Collection function: Locates the position of the first occurrence of the given value\n        in the given array. Returns null if either of the arguments are null.\n        \n        .. versionadded:: 2.4.0\n        \n        Notes\n        -----\n        The position is not zero based, but 1 based index. Returns 0 if the given\n        value could not be found in the array.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([&#34;c&#34;, &#34;b&#34;, &#34;a&#34;],), ([],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_position(df.data, &#34;a&#34;)).collect()\n        [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]\n    \n    array_remove(col, element)\n        Collection function: Remove all elements that equal to element from the given array.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        element :\n            element to be removed from the array\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_remove(df.data, 1)).collect()\n        [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]\n    \n    array_repeat(col, count)\n        Collection function: creates an array containing a column repeated count times.\n        \n        .. versionadded:: 2.4.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;ab&#39;,)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_repeat(df.data, 3).alias(&#39;r&#39;)).collect()\n        [Row(r=[&#39;ab&#39;, &#39;ab&#39;, &#39;ab&#39;])]\n    \n    array_sort(col)\n        Collection function: sorts the input array in ascending order. The elements of the input array\n        must be orderable. Null elements will be placed at the end of the returned array.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_sort(df.data).alias(&#39;r&#39;)).collect()\n        [Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]\n    \n    array_union(col1, col2)\n        Collection function: returns an array of the elements in the union of col1 and col2,\n        without duplicates.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col1 : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        col2 : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql import Row\n        &gt;&gt;&gt; df = spark.createDataFrame([Row(c1=[&#34;b&#34;, &#34;a&#34;, &#34;c&#34;], c2=[&#34;c&#34;, &#34;d&#34;, &#34;a&#34;, &#34;f&#34;])])\n        &gt;&gt;&gt; df.select(array_union(df.c1, df.c2)).collect()\n        [Row(array_union(c1, c2)=[&#39;b&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;f&#39;])]\n    \n    arrays_overlap(a1, a2)\n        Collection function: returns true if the arrays contain any common non-null element; if not,\n        returns null if both the arrays are non-empty and any of them contains a null element; returns\n        false otherwise.\n        \n        .. versionadded:: 2.4.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([&#34;a&#34;, &#34;b&#34;], [&#34;b&#34;, &#34;c&#34;]), ([&#34;a&#34;], [&#34;b&#34;, &#34;c&#34;])], [&#39;x&#39;, &#39;y&#39;])\n        &gt;&gt;&gt; df.select(arrays_overlap(df.x, df.y).alias(&#34;overlap&#34;)).collect()\n        [Row(overlap=True), Row(overlap=False)]\n    \n    arrays_zip(*cols)\n        Collection function: Returns a merged array of structs in which the N-th struct contains all\n        N-th values of input arrays.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        cols : :class:`~pyspark.sql.Column` or str\n            columns of arrays to be merged.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql.functions import arrays_zip\n        &gt;&gt;&gt; df = spark.createDataFrame([(([1, 2, 3], [2, 3, 4]))], [&#39;vals1&#39;, &#39;vals2&#39;])\n        &gt;&gt;&gt; df.select(arrays_zip(df.vals1, df.vals2).alias(&#39;zipped&#39;)).collect()\n        [Row(zipped=[Row(vals1=1, vals2=2), Row(vals1=2, vals2=3), Row(vals1=3, vals2=4)])]\n    \n    asc(col)\n        Returns a sort expression based on the ascending order of the given column name.\n        \n        .. versionadded:: 1.3\n    \n    asc_nulls_first(col)\n        Returns a sort expression based on the ascending order of the given\n        column name, and null values return before non-null values.\n        \n        .. versionadded:: 2.4\n    \n    asc_nulls_last(col)\n        Returns a sort expression based on the ascending order of the given\n        column name, and null values appear after non-null values.\n        \n        .. versionadded:: 2.4\n    \n    ascii(col)\n        Computes the numeric value of the first character of the string column.\n        \n        .. versionadded:: 1.5\n    \n    asin(col)\n        .. versionadded:: 1.3.0\n        \n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            inverse sine of `col`, as if computed by `java.lang.Math.asin()`\n    \n    asinh(col)\n        Computes inverse hyperbolic sine of the input column.\n        \n        .. versionadded:: 3.1.0\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n    \n    assert_true(col, errMsg=None)\n        Returns null if the input column is true; throws an exception with the provided error message\n        otherwise.\n        \n        .. versionadded:: 3.1.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(0,1)], [&#39;a&#39;, &#39;b&#39;])\n        &gt;&gt;&gt; df.select(assert_true(df.a &lt; df.b).alias(&#39;r&#39;)).collect()\n        [Row(r=None)]\n        &gt;&gt;&gt; df = spark.createDataFrame([(0,1)], [&#39;a&#39;, &#39;b&#39;])\n        &gt;&gt;&gt; df.select(assert_true(df.a &lt; df.b, df.a).alias(&#39;r&#39;)).collect()\n        [Row(r=None)]\n        &gt;&gt;&gt; df = spark.createDataFrame([(0,1)], [&#39;a&#39;, &#39;b&#39;])\n        &gt;&gt;&gt; df.select(assert_true(df.a &lt; df.b, &#39;error&#39;).alias(&#39;r&#39;)).collect()\n        [Row(r=None)]\n    \n    atan(col)\n        .. versionadded:: 1.4.0\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            inverse tangent of `col`, as if computed by `java.lang.Math.atan()`\n    \n    atan2(col1, col2)\n        .. versionadded:: 1.4.0\n        \n        Parameters\n        ----------\n        col1 : str, :class:`~pyspark.sql.Column` or float\n            coordinate on y-axis\n        col2 : str, :class:`~pyspark.sql.Column` or float\n            coordinate on x-axis\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            the `theta` component of the point\n            (`r`, `theta`)\n            in polar coordinates that corresponds to the point\n            (`x`, `y`) in Cartesian coordinates,\n            as if computed by `java.lang.Math.atan2()`\n    \n    atanh(col)\n        Computes inverse hyperbolic tangent of the input column.\n        \n        .. versionadded:: 3.1.0\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n    \n    avg(col)\n        Aggregate function: returns the average of the values in a group.\n        \n        .. versionadded:: 1.3\n    \n    base64(col)\n        Computes the BASE64 encoding of a binary column and returns it as a string column.\n        \n        .. versionadded:: 1.5\n    \n    bin(col)\n        Returns the string representation of the binary value of the given column.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df.select(bin(df.age).alias(&#39;c&#39;)).collect()\n        [Row(c=&#39;10&#39;), Row(c=&#39;101&#39;)]\n    \n    bitwiseNOT(col)\n        Computes bitwise not.\n        \n        .. versionadded:: 1.4\n    \n    broadcast(df)\n        Marks a DataFrame as small enough for use in broadcast joins.\n        \n        .. versionadded:: 1.6\n    \n    bround(col, scale=0)\n        Round the given value to `scale` decimal places using HALF_EVEN rounding mode if `scale` &gt;= 0\n        or at integral part when `scale` &lt; 0.\n        \n        .. versionadded:: 2.0.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; spark.createDataFrame([(2.5,)], [&#39;a&#39;]).select(bround(&#39;a&#39;, 0).alias(&#39;r&#39;)).collect()\n        [Row(r=2.0)]\n    \n    bucket(numBuckets, col)\n        Partition transform function: A transform for any type that partitions\n        by a hash of the input column.\n        \n        .. versionadded:: 3.1.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df.writeTo(&#34;catalog.db.table&#34;).partitionedBy(  # doctest: +SKIP\n        ...     bucket(42, &#34;ts&#34;)\n        ... ).createOrReplace()\n        \n        Notes\n        -----\n        This function can be used only in combination with\n        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n        method of the `DataFrameWriterV2`.\n    \n    cbrt(col)\n        Computes the cube-root of the given value.\n        \n        .. versionadded:: 1.4\n    \n    ceil(col)\n        Computes the ceiling of the given value.\n        \n        .. versionadded:: 1.4\n    \n    coalesce(*cols)\n        Returns the first column that is not null.\n        \n        .. versionadded:: 1.4.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (&#34;a&#34;, &#34;b&#34;))\n        &gt;&gt;&gt; cDf.show()\n        +----+----+\n        |   a|   b|\n        +----+----+\n        |null|null|\n        |   1|null|\n        |null|   2|\n        +----+----+\n        \n        &gt;&gt;&gt; cDf.select(coalesce(cDf[&#34;a&#34;], cDf[&#34;b&#34;])).show()\n        +--------------+\n        |coalesce(a, b)|\n        +--------------+\n        |          null|\n        |             1|\n        |             2|\n        +--------------+\n        \n        &gt;&gt;&gt; cDf.select(&#39;*&#39;, coalesce(cDf[&#34;a&#34;], lit(0.0))).show()\n        +----+----+----------------+\n        |   a|   b|coalesce(a, 0.0)|\n        +----+----+----------------+\n        |null|null|             0.0|\n        |   1|null|             1.0|\n        |null|   2|             0.0|\n        +----+----+----------------+\n    \n    col(col)\n        Returns a :class:`~pyspark.sql.Column` based on the given column name.&#39;\n        Examples\n        --------\n        &gt;&gt;&gt; col(&#39;x&#39;)\n        Column&lt;&#39;x&#39;&gt;\n        &gt;&gt;&gt; column(&#39;x&#39;)\n        Column&lt;&#39;x&#39;&gt;\n        \n        .. versionadded:: 1.3\n    \n    collect_list(col)\n        Aggregate function: returns a list of objects with duplicates.\n        \n        .. versionadded:: 1.6.0\n        \n        Notes\n        -----\n        The function is non-deterministic because the order of collected results depends\n        on the order of the rows which may be non-deterministic after a shuffle.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df2 = spark.createDataFrame([(2,), (5,), (5,)], (&#39;age&#39;,))\n        &gt;&gt;&gt; df2.agg(collect_list(&#39;age&#39;)).collect()\n        [Row(collect_list(age)=[2, 5, 5])]\n    \n    collect_set(col)\n        Aggregate function: returns a set of objects with duplicate elements eliminated.\n        \n        .. versionadded:: 1.6.0\n        \n        Notes\n        -----\n        The function is non-deterministic because the order of collected results depends\n        on the order of the rows which may be non-deterministic after a shuffle.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df2 = spark.createDataFrame([(2,), (5,), (5,)], (&#39;age&#39;,))\n        &gt;&gt;&gt; df2.agg(collect_set(&#39;age&#39;)).collect()\n        [Row(collect_set(age)=[5, 2])]\n    \n    column = col(col)\n        Returns a :class:`~pyspark.sql.Column` based on the given column name.&#39;\n        Examples\n        --------\n        &gt;&gt;&gt; col(&#39;x&#39;)\n        Column&lt;&#39;x&#39;&gt;\n        &gt;&gt;&gt; column(&#39;x&#39;)\n        Column&lt;&#39;x&#39;&gt;\n        \n        .. versionadded:: 1.3\n    \n    concat(*cols)\n        Concatenates multiple input columns together into a single column.\n        The function works with strings, binary and compatible array columns.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;abcd&#39;,&#39;123&#39;)], [&#39;s&#39;, &#39;d&#39;])\n        &gt;&gt;&gt; df.select(concat(df.s, df.d).alias(&#39;s&#39;)).collect()\n        [Row(s=&#39;abcd123&#39;)]\n        \n        &gt;&gt;&gt; df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])\n        &gt;&gt;&gt; df.select(concat(df.a, df.b, df.c).alias(&#34;arr&#34;)).collect()\n        [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n    \n    concat_ws(sep, *cols)\n        Concatenates multiple input string columns together into a single string column,\n        using the given separator.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;abcd&#39;,&#39;123&#39;)], [&#39;s&#39;, &#39;d&#39;])\n        &gt;&gt;&gt; df.select(concat_ws(&#39;-&#39;, df.s, df.d).alias(&#39;s&#39;)).collect()\n        [Row(s=&#39;abcd-123&#39;)]\n    \n    conv(col, fromBase, toBase)\n        Convert a number in a string column from one base to another.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#34;010101&#34;,)], [&#39;n&#39;])\n        &gt;&gt;&gt; df.select(conv(df.n, 2, 16).alias(&#39;hex&#39;)).collect()\n        [Row(hex=&#39;15&#39;)]\n    \n    corr(col1, col2)\n        Returns a new :class:`~pyspark.sql.Column` for the Pearson Correlation Coefficient for\n        ``col1`` and ``col2``.\n        \n        .. versionadded:: 1.6.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; a = range(20)\n        &gt;&gt;&gt; b = [2 * x for x in range(20)]\n        &gt;&gt;&gt; df = spark.createDataFrame(zip(a, b), [&#34;a&#34;, &#34;b&#34;])\n        &gt;&gt;&gt; df.agg(corr(&#34;a&#34;, &#34;b&#34;).alias(&#39;c&#39;)).collect()\n        [Row(c=1.0)]\n    \n    cos(col)\n        .. versionadded:: 1.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            angle in radians\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            cosine of the angle, as if computed by `java.lang.Math.cos()`.\n    \n    cosh(col)\n        .. versionadded:: 1.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            hyperbolic angle\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh()`\n    \n    count(col)\n        Aggregate function: returns the number of items in a group.\n        \n        .. versionadded:: 1.3\n    \n    countDistinct(col, *cols)\n        Returns a new :class:`~pyspark.sql.Column` for distinct count of ``col`` or ``cols``.\n        \n        .. versionadded:: 1.3.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df.agg(countDistinct(df.age, df.name).alias(&#39;c&#39;)).collect()\n        [Row(c=2)]\n        \n        &gt;&gt;&gt; df.agg(countDistinct(&#34;age&#34;, &#34;name&#34;).alias(&#39;c&#39;)).collect()\n        [Row(c=2)]\n    \n    covar_pop(col1, col2)\n        Returns a new :class:`~pyspark.sql.Column` for the population covariance of ``col1`` and\n        ``col2``.\n        \n        .. versionadded:: 2.0.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; a = [1] * 10\n        &gt;&gt;&gt; b = [1] * 10\n        &gt;&gt;&gt; df = spark.createDataFrame(zip(a, b), [&#34;a&#34;, &#34;b&#34;])\n        &gt;&gt;&gt; df.agg(covar_pop(&#34;a&#34;, &#34;b&#34;).alias(&#39;c&#39;)).collect()\n        [Row(c=0.0)]\n    \n    covar_samp(col1, col2)\n        Returns a new :class:`~pyspark.sql.Column` for the sample covariance of ``col1`` and\n        ``col2``.\n        \n        .. versionadded:: 2.0.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; a = [1] * 10\n        &gt;&gt;&gt; b = [1] * 10\n        &gt;&gt;&gt; df = spark.createDataFrame(zip(a, b), [&#34;a&#34;, &#34;b&#34;])\n        &gt;&gt;&gt; df.agg(covar_samp(&#34;a&#34;, &#34;b&#34;).alias(&#39;c&#39;)).collect()\n        [Row(c=0.0)]\n    \n    crc32(col)\n        Calculates the cyclic redundancy check value  (CRC32) of a binary column and\n        returns the value as a bigint.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; spark.createDataFrame([(&#39;ABC&#39;,)], [&#39;a&#39;]).select(crc32(&#39;a&#39;).alias(&#39;crc32&#39;)).collect()\n        [Row(crc32=2743272264)]\n    \n    create_map(*cols)\n\n*** WARNING: skipped 74972 bytes of output ***\n\n        [Row(s=&#39;a.b&#39;)]\n        &gt;&gt;&gt; df.select(substring_index(df.s, &#39;.&#39;, -3).alias(&#39;s&#39;)).collect()\n        [Row(s=&#39;b.c.d&#39;)]\n    \n    sum(col)\n        Aggregate function: returns the sum of all values in the expression.\n        \n        .. versionadded:: 1.3\n    \n    sumDistinct(col)\n        Aggregate function: returns the sum of distinct values in the expression.\n        \n        .. versionadded:: 1.3\n    \n    tan(col)\n        .. versionadded:: 1.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            angle in radians\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            tangent of the given value, as if computed by `java.lang.Math.tan()`\n    \n    tanh(col)\n        .. versionadded:: 1.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            hyperbolic angle\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            hyperbolic tangent of the given value\n            as if computed by `java.lang.Math.tanh()`\n    \n    timestamp_seconds(col)\n        .. versionadded:: 3.1.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql.functions import timestamp_seconds\n        &gt;&gt;&gt; spark.conf.set(&#34;spark.sql.session.timeZone&#34;, &#34;America/Los_Angeles&#34;)\n        &gt;&gt;&gt; time_df = spark.createDataFrame([(1230219000,)], [&#39;unix_time&#39;])\n        &gt;&gt;&gt; time_df.select(timestamp_seconds(time_df.unix_time).alias(&#39;ts&#39;)).show()\n        +-------------------+\n        |                 ts|\n        +-------------------+\n        |2008-12-25 07:30:00|\n        +-------------------+\n        &gt;&gt;&gt; spark.conf.unset(&#34;spark.sql.session.timeZone&#34;)\n    \n    toDegrees(col)\n        .. deprecated:: 2.1.0\n            Use :func:`degrees` instead.\n        \n        .. versionadded:: 1.4\n    \n    toRadians(col)\n        .. deprecated:: 2.1.0\n            Use :func:`radians` instead.\n        \n        .. versionadded:: 1.4\n    \n    to_csv(col, options={})\n        Converts a column containing a :class:`StructType` into a CSV string.\n        Throws an exception, in the case of an unsupported type.\n        \n        .. versionadded:: 3.0.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column containing a struct.\n        options: dict, optional\n            options to control converting. accepts the same options as the CSV datasource.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql import Row\n        &gt;&gt;&gt; data = [(1, Row(age=2, name=&#39;Alice&#39;))]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, (&#34;key&#34;, &#34;value&#34;))\n        &gt;&gt;&gt; df.select(to_csv(df.value).alias(&#34;csv&#34;)).collect()\n        [Row(csv=&#39;2,Alice&#39;)]\n    \n    to_date(col, format=None)\n        Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n        using the optionally specified format. Specify formats according to `datetime pattern`_.\n        By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n        is omitted. Equivalent to ``col.cast(&#34;date&#34;)``.\n        \n        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n        \n        .. versionadded:: 2.2.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;,)], [&#39;t&#39;])\n        &gt;&gt;&gt; df.select(to_date(df.t).alias(&#39;date&#39;)).collect()\n        [Row(date=datetime.date(1997, 2, 28))]\n        \n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;,)], [&#39;t&#39;])\n        &gt;&gt;&gt; df.select(to_date(df.t, &#39;yyyy-MM-dd HH:mm:ss&#39;).alias(&#39;date&#39;)).collect()\n        [Row(date=datetime.date(1997, 2, 28))]\n    \n    to_json(col, options={})\n        Converts a column containing a :class:`StructType`, :class:`ArrayType` or a :class:`MapType`\n        into a JSON string. Throws an exception, in the case of an unsupported type.\n        \n        .. versionadded:: 2.1.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column containing a struct, an array or a map.\n        options : dict, optional\n            options to control converting. accepts the same options as the JSON datasource.\n            Additionally the function supports the `pretty` option which enables\n            pretty JSON generation.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql import Row\n        &gt;&gt;&gt; from pyspark.sql.types import *\n        &gt;&gt;&gt; data = [(1, Row(age=2, name=&#39;Alice&#39;))]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, (&#34;key&#34;, &#34;value&#34;))\n        &gt;&gt;&gt; df.select(to_json(df.value).alias(&#34;json&#34;)).collect()\n        [Row(json=&#39;{&#34;age&#34;:2,&#34;name&#34;:&#34;Alice&#34;}&#39;)]\n        &gt;&gt;&gt; data = [(1, [Row(age=2, name=&#39;Alice&#39;), Row(age=3, name=&#39;Bob&#39;)])]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, (&#34;key&#34;, &#34;value&#34;))\n        &gt;&gt;&gt; df.select(to_json(df.value).alias(&#34;json&#34;)).collect()\n        [Row(json=&#39;[{&#34;age&#34;:2,&#34;name&#34;:&#34;Alice&#34;},{&#34;age&#34;:3,&#34;name&#34;:&#34;Bob&#34;}]&#39;)]\n        &gt;&gt;&gt; data = [(1, {&#34;name&#34;: &#34;Alice&#34;})]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, (&#34;key&#34;, &#34;value&#34;))\n        &gt;&gt;&gt; df.select(to_json(df.value).alias(&#34;json&#34;)).collect()\n        [Row(json=&#39;{&#34;name&#34;:&#34;Alice&#34;}&#39;)]\n        &gt;&gt;&gt; data = [(1, [{&#34;name&#34;: &#34;Alice&#34;}, {&#34;name&#34;: &#34;Bob&#34;}])]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, (&#34;key&#34;, &#34;value&#34;))\n        &gt;&gt;&gt; df.select(to_json(df.value).alias(&#34;json&#34;)).collect()\n        [Row(json=&#39;[{&#34;name&#34;:&#34;Alice&#34;},{&#34;name&#34;:&#34;Bob&#34;}]&#39;)]\n        &gt;&gt;&gt; data = [(1, [&#34;Alice&#34;, &#34;Bob&#34;])]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, (&#34;key&#34;, &#34;value&#34;))\n        &gt;&gt;&gt; df.select(to_json(df.value).alias(&#34;json&#34;)).collect()\n        [Row(json=&#39;[&#34;Alice&#34;,&#34;Bob&#34;]&#39;)]\n    \n    to_timestamp(col, format=None)\n        Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.TimestampType`\n        using the optionally specified format. Specify formats according to `datetime pattern`_.\n        By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n        is omitted. Equivalent to ``col.cast(&#34;timestamp&#34;)``.\n        \n        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n        \n        .. versionadded:: 2.2.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;,)], [&#39;t&#39;])\n        &gt;&gt;&gt; df.select(to_timestamp(df.t).alias(&#39;dt&#39;)).collect()\n        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n        \n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;,)], [&#39;t&#39;])\n        &gt;&gt;&gt; df.select(to_timestamp(df.t, &#39;yyyy-MM-dd HH:mm:ss&#39;).alias(&#39;dt&#39;)).collect()\n        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n    \n    to_utc_timestamp(timestamp, tz)\n        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given\n        timezone, and renders that timestamp as a timestamp in UTC.\n        \n        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n        timezone-agnostic. So in Spark this function just shift the timestamp value from the given\n        timezone to UTC timezone.\n        \n        This function may return confusing result if the input is a string with timezone, e.g.\n        &#39;2018-03-13T06:18:23+00:00&#39;. The reason is that, Spark firstly cast the string to timestamp\n        according to the timezone in the string, and finally display the result by converting the\n        timestamp to string according to the session local timezone.\n        \n        .. versionadded:: 1.5.0\n        \n        Parameters\n        ----------\n        timestamp : :class:`~pyspark.sql.Column` or str\n            the column that contains timestamps\n        tz : :class:`~pyspark.sql.Column` or str\n            A string detailing the time zone ID that the input should be adjusted to. It should\n            be in the format of either region-based zone IDs or zone offsets. Region IDs must\n            have the form &#39;area/city&#39;, such as &#39;America/Los_Angeles&#39;. Zone offsets must be in\n            the format &#39;(+|-)HH:mm&#39;, for example &#39;-08:00&#39; or &#39;+01:00&#39;. Also &#39;UTC&#39; and &#39;Z&#39; are\n            upported as aliases of &#39;+00:00&#39;. Other short names are not recommended to use\n            because they can be ambiguous.\n        \n            .. versionchanged:: 2.4.0\n               `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;, &#39;JST&#39;)], [&#39;ts&#39;, &#39;tz&#39;])\n        &gt;&gt;&gt; df.select(to_utc_timestamp(df.ts, &#34;PST&#34;).alias(&#39;utc_time&#39;)).collect()\n        [Row(utc_time=datetime.datetime(1997, 2, 28, 18, 30))]\n        &gt;&gt;&gt; df.select(to_utc_timestamp(df.ts, df.tz).alias(&#39;utc_time&#39;)).collect()\n        [Row(utc_time=datetime.datetime(1997, 2, 28, 1, 30))]\n    \n    transform(col, f)\n        Returns an array of elements after applying a transformation to each element in the input array.\n        \n        .. versionadded:: 3.1.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        f : function\n            a function that is applied to each element of the input array.\n            Can take one of the following forms:\n        \n            - Unary ``(x: Column) -&gt; Column: ...``\n            - Binary ``(x: Column, i: Column) -&gt; Column...``, where the second argument is\n                a 0-based index of the element.\n        \n            and can use methods of :class:`~pyspark.sql.Column`, functions defined in\n            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n            Python ``UserDefinedFunctions`` are not supported\n            (`SPARK-27052 &lt;https://issues.apache.org/jira/browse/SPARK-27052&gt;`__).\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, [1, 2, 3, 4])], (&#34;key&#34;, &#34;values&#34;))\n        &gt;&gt;&gt; df.select(transform(&#34;values&#34;, lambda x: x * 2).alias(&#34;doubled&#34;)).show()\n        +------------+\n        |     doubled|\n        +------------+\n        |[2, 4, 6, 8]|\n        +------------+\n        \n        &gt;&gt;&gt; def alternate(x, i):\n        ...     return when(i % 2 == 0, x).otherwise(-x)\n        &gt;&gt;&gt; df.select(transform(&#34;values&#34;, alternate).alias(&#34;alternated&#34;)).show()\n        +--------------+\n        |    alternated|\n        +--------------+\n        |[1, -2, 3, -4]|\n        +--------------+\n    \n    transform_keys(col, f)\n        Applies a function to every key-value pair in a map and returns\n        a map with the results of those applications as the new keys for the pairs.\n        \n        .. versionadded:: 3.1.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        f : function\n            a binary function ``(k: Column, v: Column) -&gt; Column...``\n            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n            Python ``UserDefinedFunctions`` are not supported\n            (`SPARK-27052 &lt;https://issues.apache.org/jira/browse/SPARK-27052&gt;`__).\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, {&#34;foo&#34;: -2.0, &#34;bar&#34;: 2.0})], (&#34;id&#34;, &#34;data&#34;))\n        &gt;&gt;&gt; df.select(transform_keys(\n        ...     &#34;data&#34;, lambda k, _: upper(k)).alias(&#34;data_upper&#34;)\n        ... ).show(truncate=False)\n        +-------------------------+\n        |data_upper               |\n        +-------------------------+\n        |{BAR -&gt; 2.0, FOO -&gt; -2.0}|\n        +-------------------------+\n    \n    transform_values(col, f)\n        Applies a function to every key-value pair in a map and returns\n        a map with the results of those applications as the new values for the pairs.\n        \n        .. versionadded:: 3.1.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        f : function\n            a binary function ``(k: Column, v: Column) -&gt; Column...``\n            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n            Python ``UserDefinedFunctions`` are not supported\n            (`SPARK-27052 &lt;https://issues.apache.org/jira/browse/SPARK-27052&gt;`__).\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, {&#34;IT&#34;: 10.0, &#34;SALES&#34;: 2.0, &#34;OPS&#34;: 24.0})], (&#34;id&#34;, &#34;data&#34;))\n        &gt;&gt;&gt; df.select(transform_values(\n        ...     &#34;data&#34;, lambda k, v: when(k.isin(&#34;IT&#34;, &#34;OPS&#34;), v + 10.0).otherwise(v)\n        ... ).alias(&#34;new_data&#34;)).show(truncate=False)\n        +---------------------------------------+\n        |new_data                               |\n        +---------------------------------------+\n        |{OPS -&gt; 34.0, IT -&gt; 20.0, SALES -&gt; 2.0}|\n        +---------------------------------------+\n    \n    translate(srcCol, matching, replace)\n        A function translate any character in the `srcCol` by a character in `matching`.\n        The characters in `replace` is corresponding to the characters in `matching`.\n        The translate will happen when any character in the string matching with the character\n        in the `matching`.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; spark.createDataFrame([(&#39;translate&#39;,)], [&#39;a&#39;]).select(translate(&#39;a&#39;, &#34;rnlt&#34;, &#34;123&#34;) \\\n        ...     .alias(&#39;r&#39;)).collect()\n        [Row(r=&#39;1a2s3ae&#39;)]\n    \n    trim(col)\n        Trim the spaces from both ends for the specified string column.\n        \n        .. versionadded:: 1.5\n    \n    trunc(date, format)\n        Returns date truncated to the unit specified by the format.\n        \n        .. versionadded:: 1.5.0\n        \n        Parameters\n        ----------\n        date : :class:`~pyspark.sql.Column` or str\n        format : str\n            &#39;year&#39;, &#39;yyyy&#39;, &#39;yy&#39; to truncate by year,\n            or &#39;month&#39;, &#39;mon&#39;, &#39;mm&#39; to truncate by month\n            Other options are: &#39;week&#39;, &#39;quarter&#39;\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28&#39;,)], [&#39;d&#39;])\n        &gt;&gt;&gt; df.select(trunc(df.d, &#39;year&#39;).alias(&#39;year&#39;)).collect()\n        [Row(year=datetime.date(1997, 1, 1))]\n        &gt;&gt;&gt; df.select(trunc(df.d, &#39;mon&#39;).alias(&#39;month&#39;)).collect()\n        [Row(month=datetime.date(1997, 2, 1))]\n    \n    udf(f=None, returnType=StringType)\n        Creates a user defined function (UDF).\n        \n        .. versionadded:: 1.3.0\n        \n        Parameters\n        ----------\n        f : function\n            python function if used as a standalone function\n        returnType : :class:`pyspark.sql.types.DataType` or str\n            the return type of the user-defined function. The value can be either a\n            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; slen = udf(lambda s: len(s), IntegerType())\n        &gt;&gt;&gt; @udf\n        ... def to_upper(s):\n        ...     if s is not None:\n        ...         return s.upper()\n        ...\n        &gt;&gt;&gt; @udf(returnType=IntegerType())\n        ... def add_one(x):\n        ...     if x is not None:\n        ...         return x + 1\n        ...\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, &#34;John Doe&#34;, 21)], (&#34;id&#34;, &#34;name&#34;, &#34;age&#34;))\n        &gt;&gt;&gt; df.select(slen(&#34;name&#34;).alias(&#34;slen(name)&#34;), to_upper(&#34;name&#34;), add_one(&#34;age&#34;)).show()\n        +----------+--------------+------------+\n        |slen(name)|to_upper(name)|add_one(age)|\n        +----------+--------------+------------+\n        |         8|      JOHN DOE|          22|\n        +----------+--------------+------------+\n        \n        Notes\n        -----\n        The user-defined functions are considered deterministic by default. Due to\n        optimization, duplicate invocations may be eliminated or the function may even be invoked\n        more times than it is present in the query. If your function is not deterministic, call\n        `asNondeterministic` on the user defined function. E.g.:\n        \n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; import random\n        &gt;&gt;&gt; random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n        \n        The user-defined functions do not support conditional expressions or short circuiting\n        in boolean expressions and it ends up with being executed all internally. If the functions\n        can fail on special rows, the workaround is to incorporate the condition into the functions.\n        \n        The user-defined functions do not take keyword arguments on the calling side.\n    \n    unbase64(col)\n        Decodes a BASE64 encoded string column and returns it as a binary column.\n        \n        .. versionadded:: 1.5\n    \n    unhex(col)\n        Inverse of hex. Interprets each pair of characters as a hexadecimal number\n        and converts to the byte representation of number.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; spark.createDataFrame([(&#39;414243&#39;,)], [&#39;a&#39;]).select(unhex(&#39;a&#39;)).collect()\n        [Row(unhex(a)=bytearray(b&#39;ABC&#39;))]\n    \n    unix_timestamp(timestamp=None, format=&#39;yyyy-MM-dd HH:mm:ss&#39;)\n        Convert time string with given pattern (&#39;yyyy-MM-dd HH:mm:ss&#39;, by default)\n        to Unix time stamp (in seconds), using the default timezone and the default\n        locale, return null if fail.\n        \n        if `timestamp` is None, then it returns current timestamp.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; spark.conf.set(&#34;spark.sql.session.timeZone&#34;, &#34;America/Los_Angeles&#34;)\n        &gt;&gt;&gt; time_df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])\n        &gt;&gt;&gt; time_df.select(unix_timestamp(&#39;dt&#39;, &#39;yyyy-MM-dd&#39;).alias(&#39;unix_time&#39;)).collect()\n        [Row(unix_time=1428476400)]\n        &gt;&gt;&gt; spark.conf.unset(&#34;spark.sql.session.timeZone&#34;)\n    \n    upper(col)\n        Converts a string expression to upper case.\n        \n        .. versionadded:: 1.5\n    \n    var_pop(col)\n        Aggregate function: returns the population variance of the values in a group.\n        \n        .. versionadded:: 1.6\n    \n    var_samp(col)\n        Aggregate function: returns the unbiased sample variance of\n        the values in a group.\n        \n        .. versionadded:: 1.6\n    \n    variance(col)\n        Aggregate function: alias for var_samp\n        \n        .. versionadded:: 1.6\n    \n    weekofyear(col)\n        Extract the week number of a given date as integer.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])\n        &gt;&gt;&gt; df.select(weekofyear(df.dt).alias(&#39;week&#39;)).collect()\n        [Row(week=15)]\n    \n    when(condition, value)\n        Evaluates a list of conditions and returns one of multiple possible result expressions.\n        If :func:`pyspark.sql.Column.otherwise` is not invoked, None is returned for unmatched\n        conditions.\n        \n        .. versionadded:: 1.4.0\n        \n        Parameters\n        ----------\n        condition : :class:`~pyspark.sql.Column`\n            a boolean :class:`~pyspark.sql.Column` expression.\n        value :\n            a literal value, or a :class:`~pyspark.sql.Column` expression.\n        \n        &gt;&gt;&gt; df.select(when(df[&#39;age&#39;] == 2, 3).otherwise(4).alias(&#34;age&#34;)).collect()\n        [Row(age=3), Row(age=4)]\n        \n        &gt;&gt;&gt; df.select(when(df.age == 2, df.age + 1).alias(&#34;age&#34;)).collect()\n        [Row(age=3), Row(age=None)]\n    \n    window(timeColumn, windowDuration, slideDuration=None, startTime=None)\n        Bucketize rows into one or more time windows given a timestamp specifying column. Window\n        starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n        [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\n        the order of months are not supported.\n        \n        The time column must be of :class:`pyspark.sql.types.TimestampType`.\n        \n        Durations are provided as strings, e.g. &#39;1 second&#39;, &#39;1 day 12 hours&#39;, &#39;2 minutes&#39;. Valid\n        interval strings are &#39;week&#39;, &#39;day&#39;, &#39;hour&#39;, &#39;minute&#39;, &#39;second&#39;, &#39;millisecond&#39;, &#39;microsecond&#39;.\n        If the ``slideDuration`` is not provided, the windows will be tumbling windows.\n        \n        The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\n        window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\n        past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\n        \n        The output column will be a struct called &#39;window&#39; by default with the nested columns &#39;start&#39;\n        and &#39;end&#39;, where &#39;start&#39; and &#39;end&#39; will be of :class:`pyspark.sql.types.TimestampType`.\n        \n        .. versionadded:: 2.0.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#34;2016-03-11 09:00:07&#34;, 1)]).toDF(&#34;date&#34;, &#34;val&#34;)\n        &gt;&gt;&gt; w = df.groupBy(window(&#34;date&#34;, &#34;5 seconds&#34;)).agg(sum(&#34;val&#34;).alias(&#34;sum&#34;))\n        &gt;&gt;&gt; w.select(w.window.start.cast(&#34;string&#34;).alias(&#34;start&#34;),\n        ...          w.window.end.cast(&#34;string&#34;).alias(&#34;end&#34;), &#34;sum&#34;).collect()\n        [Row(start=&#39;2016-03-11 09:00:05&#39;, end=&#39;2016-03-11 09:00:10&#39;, sum=1)]\n    \n    xxhash64(*cols)\n        Calculates the hash code of given columns using the 64-bit variant of the xxHash algorithm,\n        and returns the result as a long column.\n        \n        .. versionadded:: 3.0.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; spark.createDataFrame([(&#39;ABC&#39;,)], [&#39;a&#39;]).select(xxhash64(&#39;a&#39;).alias(&#39;hash&#39;)).collect()\n        [Row(hash=4105715581806190027)]\n    \n    year(col)\n        Extract the year of a given date as integer.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])\n        &gt;&gt;&gt; df.select(year(&#39;dt&#39;).alias(&#39;year&#39;)).collect()\n        [Row(year=2015)]\n    \n    years(col)\n        Partition transform function: A transform for timestamps and dates\n        to partition data into years.\n        \n        .. versionadded:: 3.1.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df.writeTo(&#34;catalog.db.table&#34;).partitionedBy(  # doctest: +SKIP\n        ...     years(&#34;ts&#34;)\n        ... ).createOrReplace()\n        \n        Notes\n        -----\n        This function can be used only in combination with\n        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n        method of the `DataFrameWriterV2`.\n    \n    zip_with(left, right, f)\n        Merge two given arrays, element-wise, into a single array using a function.\n        If one array is shorter, nulls are appended at the end to match the length of the longer\n        array, before applying the function.\n        \n        .. versionadded:: 3.1.0\n        \n        Parameters\n        ----------\n        left : :class:`~pyspark.sql.Column` or str\n            name of the first column or expression\n        right : :class:`~pyspark.sql.Column` or str\n            name of the second column or expression\n        f : function\n            a binary function ``(x1: Column, x2: Column) -&gt; Column...``\n            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n            Python ``UserDefinedFunctions`` are not supported\n            (`SPARK-27052 &lt;https://issues.apache.org/jira/browse/SPARK-27052&gt;`__).\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, [1, 3, 5, 8], [0, 2, 4, 6])], (&#34;id&#34;, &#34;xs&#34;, &#34;ys&#34;))\n        &gt;&gt;&gt; df.select(zip_with(&#34;xs&#34;, &#34;ys&#34;, lambda x, y: x ** y).alias(&#34;powers&#34;)).show(truncate=False)\n        +---------------------------+\n        |powers                     |\n        +---------------------------+\n        |[1.0, 9.0, 625.0, 262144.0]|\n        +---------------------------+\n        \n        &gt;&gt;&gt; df = spark.createDataFrame([(1, [&#34;foo&#34;, &#34;bar&#34;], [1, 2, 3])], (&#34;id&#34;, &#34;xs&#34;, &#34;ys&#34;))\n        &gt;&gt;&gt; df.select(zip_with(&#34;xs&#34;, &#34;ys&#34;, lambda x, y: concat_ws(&#34;_&#34;, x, y)).alias(&#34;xs_ys&#34;)).show()\n        +-----------------+\n        |            xs_ys|\n        +-----------------+\n        |[foo_1, bar_2, 3]|\n        +-----------------+\n\nFILE\n    /databricks/spark/python/pyspark/sql/functions.py\n\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on module pyspark.sql.functions in pyspark.sql:\n\nNAME\n    pyspark.sql.functions - A collections of builtin functions\n\nFUNCTIONS\n    abs(col)\n        Computes the absolute value.\n        \n        .. versionadded:: 1.3\n    \n    acos(col)\n        .. versionadded:: 1.4.0\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            inverse cosine of `col`, as if computed by `java.lang.Math.acos()`\n    \n    acosh(col)\n        Computes inverse hyperbolic cosine of the input column.\n        \n        .. versionadded:: 3.1.0\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n    \n    add_months(start, months)\n        Returns the date that is `months` months after `start`\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])\n        &gt;&gt;&gt; df.select(add_months(df.dt, 1).alias(&#39;next_month&#39;)).collect()\n        [Row(next_month=datetime.date(2015, 5, 8))]\n    \n    aggregate(col, initialValue, merge, finish=None)\n        Applies a binary operator to an initial state and all elements in the array,\n        and reduces this to a single state. The final state is converted into the final result\n        by applying a finish function.\n        \n        Both functions can use methods of :class:`~pyspark.sql.Column`, functions defined in\n        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n        Python ``UserDefinedFunctions`` are not supported\n        (`SPARK-27052 &lt;https://issues.apache.org/jira/browse/SPARK-27052&gt;`__).\n        \n        .. versionadded:: 3.1.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        initialValue : :class:`~pyspark.sql.Column` or str\n            initial value. Name of column or expression\n        merge : function\n            a binary function ``(acc: Column, x: Column) -&gt; Column...`` returning expression\n            of the same type as ``zero``\n        finish : function\n            an optional unary function ``(x: Column) -&gt; Column: ...``\n            used to convert accumulated value.\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (&#34;id&#34;, &#34;values&#34;))\n        &gt;&gt;&gt; df.select(aggregate(&#34;values&#34;, lit(0.0), lambda acc, x: acc + x).alias(&#34;sum&#34;)).show()\n        +----+\n sum|\n        +----+\n42.0|\n        +----+\n        \n        &gt;&gt;&gt; def merge(acc, x):\n        ...     count = acc.count + 1\n        ...     sum = acc.sum + x\n        ...     return struct(count.alias(&#34;count&#34;), sum.alias(&#34;sum&#34;))\n        &gt;&gt;&gt; df.select(\n        ...     aggregate(\n        ...         &#34;values&#34;,\n        ...         struct(lit(0).alias(&#34;count&#34;), lit(0.0).alias(&#34;sum&#34;)),\n        ...         merge,\n        ...         lambda acc: acc.sum / acc.count,\n        ...     ).alias(&#34;mean&#34;)\n        ... ).show()\n        +----+\nmean|\n        +----+\n 8.4|\n        +----+\n    \n    approxCountDistinct(col, rsd=None)\n        .. deprecated:: 2.1.0\n            Use :func:`approx_count_distinct` instead.\n        \n        .. versionadded:: 1.3\n    \n    approx_count_distinct(col, rsd=None)\n        Aggregate function: returns a new :class:`~pyspark.sql.Column` for approximate distinct count\n        of column `col`.\n        \n        .. versionadded:: 2.1.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n        rsd : float, optional\n            maximum relative standard deviation allowed (default = 0.05).\n            For rsd &lt; 0.01, it is more efficient to use :func:`countDistinct`\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df.agg(approx_count_distinct(df.age).alias(&#39;distinct_ages&#39;)).collect()\n        [Row(distinct_ages=2)]\n    \n    array(*cols)\n        Creates a new array column.\n        \n        .. versionadded:: 1.4.0\n        \n        Parameters\n        ----------\n        cols : :class:`~pyspark.sql.Column` or str\n            column names or :class:`~pyspark.sql.Column`\\s that have\n            the same data type.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df.select(array(&#39;age&#39;, &#39;age&#39;).alias(&#34;arr&#34;)).collect()\n        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n        &gt;&gt;&gt; df.select(array([df.age, df.age]).alias(&#34;arr&#34;)).collect()\n        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n    \n    array_contains(col, value)\n        Collection function: returns null if the array is null, true if the array contains the\n        given value, and false otherwise.\n        \n        .. versionadded:: 1.5.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        value :\n            value or column to check for in array\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([&#34;a&#34;, &#34;b&#34;, &#34;c&#34;],), ([],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_contains(df.data, &#34;a&#34;)).collect()\n        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n        &gt;&gt;&gt; df.select(array_contains(df.data, lit(&#34;a&#34;))).collect()\n        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n    \n    array_distinct(col)\n        Collection function: removes duplicate values from the array.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_distinct(df.data)).collect()\n        [Row(array_distinct(data)=[1, 2, 3]), Row(array_distinct(data)=[4, 5])]\n    \n    array_except(col1, col2)\n        Collection function: returns an array of the elements in col1 but not in col2,\n        without duplicates.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col1 : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        col2 : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql import Row\n        &gt;&gt;&gt; df = spark.createDataFrame([Row(c1=[&#34;b&#34;, &#34;a&#34;, &#34;c&#34;], c2=[&#34;c&#34;, &#34;d&#34;, &#34;a&#34;, &#34;f&#34;])])\n        &gt;&gt;&gt; df.select(array_except(df.c1, df.c2)).collect()\n        [Row(array_except(c1, c2)=[&#39;b&#39;])]\n    \n    array_intersect(col1, col2)\n        Collection function: returns an array of the elements in the intersection of col1 and col2,\n        without duplicates.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col1 : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        col2 : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql import Row\n        &gt;&gt;&gt; df = spark.createDataFrame([Row(c1=[&#34;b&#34;, &#34;a&#34;, &#34;c&#34;], c2=[&#34;c&#34;, &#34;d&#34;, &#34;a&#34;, &#34;f&#34;])])\n        &gt;&gt;&gt; df.select(array_intersect(df.c1, df.c2)).collect()\n        [Row(array_intersect(c1, c2)=[&#39;a&#39;, &#39;c&#39;])]\n    \n    array_join(col, delimiter, null_replacement=None)\n        Concatenates the elements of `column` using the `delimiter`. Null values are replaced with\n        `null_replacement` if set, otherwise they are ignored.\n        \n        .. versionadded:: 2.4.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([&#34;a&#34;, &#34;b&#34;, &#34;c&#34;],), ([&#34;a&#34;, None],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_join(df.data, &#34;,&#34;).alias(&#34;joined&#34;)).collect()\n        [Row(joined=&#39;a,b,c&#39;), Row(joined=&#39;a&#39;)]\n        &gt;&gt;&gt; df.select(array_join(df.data, &#34;,&#34;, &#34;NULL&#34;).alias(&#34;joined&#34;)).collect()\n        [Row(joined=&#39;a,b,c&#39;), Row(joined=&#39;a,NULL&#39;)]\n    \n    array_max(col)\n        Collection function: returns the maximum value of the array.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_max(df.data).alias(&#39;max&#39;)).collect()\n        [Row(max=3), Row(max=10)]\n    \n    array_min(col)\n        Collection function: returns the minimum value of the array.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_min(df.data).alias(&#39;min&#39;)).collect()\n        [Row(min=1), Row(min=-1)]\n    \n    array_position(col, value)\n        Collection function: Locates the position of the first occurrence of the given value\n        in the given array. Returns null if either of the arguments are null.\n        \n        .. versionadded:: 2.4.0\n        \n        Notes\n        -----\n        The position is not zero based, but 1 based index. Returns 0 if the given\n        value could not be found in the array.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([&#34;c&#34;, &#34;b&#34;, &#34;a&#34;],), ([],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_position(df.data, &#34;a&#34;)).collect()\n        [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]\n    \n    array_remove(col, element)\n        Collection function: Remove all elements that equal to element from the given array.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        element :\n            element to be removed from the array\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_remove(df.data, 1)).collect()\n        [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]\n    \n    array_repeat(col, count)\n        Collection function: creates an array containing a column repeated count times.\n        \n        .. versionadded:: 2.4.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;ab&#39;,)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_repeat(df.data, 3).alias(&#39;r&#39;)).collect()\n        [Row(r=[&#39;ab&#39;, &#39;ab&#39;, &#39;ab&#39;])]\n    \n    array_sort(col)\n        Collection function: sorts the input array in ascending order. The elements of the input array\n        must be orderable. Null elements will be placed at the end of the returned array.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], [&#39;data&#39;])\n        &gt;&gt;&gt; df.select(array_sort(df.data).alias(&#39;r&#39;)).collect()\n        [Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]\n    \n    array_union(col1, col2)\n        Collection function: returns an array of the elements in the union of col1 and col2,\n        without duplicates.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        col1 : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        col2 : :class:`~pyspark.sql.Column` or str\n            name of column containing array\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql import Row\n        &gt;&gt;&gt; df = spark.createDataFrame([Row(c1=[&#34;b&#34;, &#34;a&#34;, &#34;c&#34;], c2=[&#34;c&#34;, &#34;d&#34;, &#34;a&#34;, &#34;f&#34;])])\n        &gt;&gt;&gt; df.select(array_union(df.c1, df.c2)).collect()\n        [Row(array_union(c1, c2)=[&#39;b&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;f&#39;])]\n    \n    arrays_overlap(a1, a2)\n        Collection function: returns true if the arrays contain any common non-null element; if not,\n        returns null if both the arrays are non-empty and any of them contains a null element; returns\n        false otherwise.\n        \n        .. versionadded:: 2.4.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([([&#34;a&#34;, &#34;b&#34;], [&#34;b&#34;, &#34;c&#34;]), ([&#34;a&#34;], [&#34;b&#34;, &#34;c&#34;])], [&#39;x&#39;, &#39;y&#39;])\n        &gt;&gt;&gt; df.select(arrays_overlap(df.x, df.y).alias(&#34;overlap&#34;)).collect()\n        [Row(overlap=True), Row(overlap=False)]\n    \n    arrays_zip(*cols)\n        Collection function: Returns a merged array of structs in which the N-th struct contains all\n        N-th values of input arrays.\n        \n        .. versionadded:: 2.4.0\n        \n        Parameters\n        ----------\n        cols : :class:`~pyspark.sql.Column` or str\n            columns of arrays to be merged.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql.functions import arrays_zip\n        &gt;&gt;&gt; df = spark.createDataFrame([(([1, 2, 3], [2, 3, 4]))], [&#39;vals1&#39;, &#39;vals2&#39;])\n        &gt;&gt;&gt; df.select(arrays_zip(df.vals1, df.vals2).alias(&#39;zipped&#39;)).collect()\n        [Row(zipped=[Row(vals1=1, vals2=2), Row(vals1=2, vals2=3), Row(vals1=3, vals2=4)])]\n    \n    asc(col)\n        Returns a sort expression based on the ascending order of the given column name.\n        \n        .. versionadded:: 1.3\n    \n    asc_nulls_first(col)\n        Returns a sort expression based on the ascending order of the given\n        column name, and null values return before non-null values.\n        \n        .. versionadded:: 2.4\n    \n    asc_nulls_last(col)\n        Returns a sort expression based on the ascending order of the given\n        column name, and null values appear after non-null values.\n        \n        .. versionadded:: 2.4\n    \n    ascii(col)\n        Computes the numeric value of the first character of the string column.\n        \n        .. versionadded:: 1.5\n    \n    asin(col)\n        .. versionadded:: 1.3.0\n        \n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            inverse sine of `col`, as if computed by `java.lang.Math.asin()`\n    \n    asinh(col)\n        Computes inverse hyperbolic sine of the input column.\n        \n        .. versionadded:: 3.1.0\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n    \n    assert_true(col, errMsg=None)\n        Returns null if the input column is true; throws an exception with the provided error message\n        otherwise.\n        \n        .. versionadded:: 3.1.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(0,1)], [&#39;a&#39;, &#39;b&#39;])\n        &gt;&gt;&gt; df.select(assert_true(df.a &lt; df.b).alias(&#39;r&#39;)).collect()\n        [Row(r=None)]\n        &gt;&gt;&gt; df = spark.createDataFrame([(0,1)], [&#39;a&#39;, &#39;b&#39;])\n        &gt;&gt;&gt; df.select(assert_true(df.a &lt; df.b, df.a).alias(&#39;r&#39;)).collect()\n        [Row(r=None)]\n        &gt;&gt;&gt; df = spark.createDataFrame([(0,1)], [&#39;a&#39;, &#39;b&#39;])\n        &gt;&gt;&gt; df.select(assert_true(df.a &lt; df.b, &#39;error&#39;).alias(&#39;r&#39;)).collect()\n        [Row(r=None)]\n    \n    atan(col)\n        .. versionadded:: 1.4.0\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            inverse tangent of `col`, as if computed by `java.lang.Math.atan()`\n    \n    atan2(col1, col2)\n        .. versionadded:: 1.4.0\n        \n        Parameters\n        ----------\n        col1 : str, :class:`~pyspark.sql.Column` or float\n            coordinate on y-axis\n        col2 : str, :class:`~pyspark.sql.Column` or float\n            coordinate on x-axis\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            the `theta` component of the point\n            (`r`, `theta`)\n            in polar coordinates that corresponds to the point\n            (`x`, `y`) in Cartesian coordinates,\n            as if computed by `java.lang.Math.atan2()`\n    \n    atanh(col)\n        Computes inverse hyperbolic tangent of the input column.\n        \n        .. versionadded:: 3.1.0\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n    \n    avg(col)\n        Aggregate function: returns the average of the values in a group.\n        \n        .. versionadded:: 1.3\n    \n    base64(col)\n        Computes the BASE64 encoding of a binary column and returns it as a string column.\n        \n        .. versionadded:: 1.5\n    \n    bin(col)\n        Returns the string representation of the binary value of the given column.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df.select(bin(df.age).alias(&#39;c&#39;)).collect()\n        [Row(c=&#39;10&#39;), Row(c=&#39;101&#39;)]\n    \n    bitwiseNOT(col)\n        Computes bitwise not.\n        \n        .. versionadded:: 1.4\n    \n    broadcast(df)\n        Marks a DataFrame as small enough for use in broadcast joins.\n        \n        .. versionadded:: 1.6\n    \n    bround(col, scale=0)\n        Round the given value to `scale` decimal places using HALF_EVEN rounding mode if `scale` &gt;= 0\n        or at integral part when `scale` &lt; 0.\n        \n        .. versionadded:: 2.0.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; spark.createDataFrame([(2.5,)], [&#39;a&#39;]).select(bround(&#39;a&#39;, 0).alias(&#39;r&#39;)).collect()\n        [Row(r=2.0)]\n    \n    bucket(numBuckets, col)\n        Partition transform function: A transform for any type that partitions\n        by a hash of the input column.\n        \n        .. versionadded:: 3.1.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df.writeTo(&#34;catalog.db.table&#34;).partitionedBy(  # doctest: +SKIP\n        ...     bucket(42, &#34;ts&#34;)\n        ... ).createOrReplace()\n        \n        Notes\n        -----\n        This function can be used only in combination with\n        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n        method of the `DataFrameWriterV2`.\n    \n    cbrt(col)\n        Computes the cube-root of the given value.\n        \n        .. versionadded:: 1.4\n    \n    ceil(col)\n        Computes the ceiling of the given value.\n        \n        .. versionadded:: 1.4\n    \n    coalesce(*cols)\n        Returns the first column that is not null.\n        \n        .. versionadded:: 1.4.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (&#34;a&#34;, &#34;b&#34;))\n        &gt;&gt;&gt; cDf.show()\n        +----+----+\n   a|   b|\n        +----+----+\nnull|null|\n   1|null|\nnull|   2|\n        +----+----+\n        \n        &gt;&gt;&gt; cDf.select(coalesce(cDf[&#34;a&#34;], cDf[&#34;b&#34;])).show()\n        +--------------+\ncoalesce(a, b)|\n        +--------------+\n          null|\n             1|\n             2|\n        +--------------+\n        \n        &gt;&gt;&gt; cDf.select(&#39;*&#39;, coalesce(cDf[&#34;a&#34;], lit(0.0))).show()\n        +----+----+----------------+\n   a|   b|coalesce(a, 0.0)|\n        +----+----+----------------+\nnull|null|             0.0|\n   1|null|             1.0|\nnull|   2|             0.0|\n        +----+----+----------------+\n    \n    col(col)\n        Returns a :class:`~pyspark.sql.Column` based on the given column name.&#39;\n        Examples\n        --------\n        &gt;&gt;&gt; col(&#39;x&#39;)\n        Column&lt;&#39;x&#39;&gt;\n        &gt;&gt;&gt; column(&#39;x&#39;)\n        Column&lt;&#39;x&#39;&gt;\n        \n        .. versionadded:: 1.3\n    \n    collect_list(col)\n        Aggregate function: returns a list of objects with duplicates.\n        \n        .. versionadded:: 1.6.0\n        \n        Notes\n        -----\n        The function is non-deterministic because the order of collected results depends\n        on the order of the rows which may be non-deterministic after a shuffle.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df2 = spark.createDataFrame([(2,), (5,), (5,)], (&#39;age&#39;,))\n        &gt;&gt;&gt; df2.agg(collect_list(&#39;age&#39;)).collect()\n        [Row(collect_list(age)=[2, 5, 5])]\n    \n    collect_set(col)\n        Aggregate function: returns a set of objects with duplicate elements eliminated.\n        \n        .. versionadded:: 1.6.0\n        \n        Notes\n        -----\n        The function is non-deterministic because the order of collected results depends\n        on the order of the rows which may be non-deterministic after a shuffle.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df2 = spark.createDataFrame([(2,), (5,), (5,)], (&#39;age&#39;,))\n        &gt;&gt;&gt; df2.agg(collect_set(&#39;age&#39;)).collect()\n        [Row(collect_set(age)=[5, 2])]\n    \n    column = col(col)\n        Returns a :class:`~pyspark.sql.Column` based on the given column name.&#39;\n        Examples\n        --------\n        &gt;&gt;&gt; col(&#39;x&#39;)\n        Column&lt;&#39;x&#39;&gt;\n        &gt;&gt;&gt; column(&#39;x&#39;)\n        Column&lt;&#39;x&#39;&gt;\n        \n        .. versionadded:: 1.3\n    \n    concat(*cols)\n        Concatenates multiple input columns together into a single column.\n        The function works with strings, binary and compatible array columns.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;abcd&#39;,&#39;123&#39;)], [&#39;s&#39;, &#39;d&#39;])\n        &gt;&gt;&gt; df.select(concat(df.s, df.d).alias(&#39;s&#39;)).collect()\n        [Row(s=&#39;abcd123&#39;)]\n        \n        &gt;&gt;&gt; df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])\n        &gt;&gt;&gt; df.select(concat(df.a, df.b, df.c).alias(&#34;arr&#34;)).collect()\n        [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n    \n    concat_ws(sep, *cols)\n        Concatenates multiple input string columns together into a single string column,\n        using the given separator.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;abcd&#39;,&#39;123&#39;)], [&#39;s&#39;, &#39;d&#39;])\n        &gt;&gt;&gt; df.select(concat_ws(&#39;-&#39;, df.s, df.d).alias(&#39;s&#39;)).collect()\n        [Row(s=&#39;abcd-123&#39;)]\n    \n    conv(col, fromBase, toBase)\n        Convert a number in a string column from one base to another.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#34;010101&#34;,)], [&#39;n&#39;])\n        &gt;&gt;&gt; df.select(conv(df.n, 2, 16).alias(&#39;hex&#39;)).collect()\n        [Row(hex=&#39;15&#39;)]\n    \n    corr(col1, col2)\n        Returns a new :class:`~pyspark.sql.Column` for the Pearson Correlation Coefficient for\n        ``col1`` and ``col2``.\n        \n        .. versionadded:: 1.6.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; a = range(20)\n        &gt;&gt;&gt; b = [2 * x for x in range(20)]\n        &gt;&gt;&gt; df = spark.createDataFrame(zip(a, b), [&#34;a&#34;, &#34;b&#34;])\n        &gt;&gt;&gt; df.agg(corr(&#34;a&#34;, &#34;b&#34;).alias(&#39;c&#39;)).collect()\n        [Row(c=1.0)]\n    \n    cos(col)\n        .. versionadded:: 1.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            angle in radians\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            cosine of the angle, as if computed by `java.lang.Math.cos()`.\n    \n    cosh(col)\n        .. versionadded:: 1.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            hyperbolic angle\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh()`\n    \n    count(col)\n        Aggregate function: returns the number of items in a group.\n        \n        .. versionadded:: 1.3\n    \n    countDistinct(col, *cols)\n        Returns a new :class:`~pyspark.sql.Column` for distinct count of ``col`` or ``cols``.\n        \n        .. versionadded:: 1.3.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df.agg(countDistinct(df.age, df.name).alias(&#39;c&#39;)).collect()\n        [Row(c=2)]\n        \n        &gt;&gt;&gt; df.agg(countDistinct(&#34;age&#34;, &#34;name&#34;).alias(&#39;c&#39;)).collect()\n        [Row(c=2)]\n    \n    covar_pop(col1, col2)\n        Returns a new :class:`~pyspark.sql.Column` for the population covariance of ``col1`` and\n        ``col2``.\n        \n        .. versionadded:: 2.0.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; a = [1] * 10\n        &gt;&gt;&gt; b = [1] * 10\n        &gt;&gt;&gt; df = spark.createDataFrame(zip(a, b), [&#34;a&#34;, &#34;b&#34;])\n        &gt;&gt;&gt; df.agg(covar_pop(&#34;a&#34;, &#34;b&#34;).alias(&#39;c&#39;)).collect()\n        [Row(c=0.0)]\n    \n    covar_samp(col1, col2)\n        Returns a new :class:`~pyspark.sql.Column` for the sample covariance of ``col1`` and\n        ``col2``.\n        \n        .. versionadded:: 2.0.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; a = [1] * 10\n        &gt;&gt;&gt; b = [1] * 10\n        &gt;&gt;&gt; df = spark.createDataFrame(zip(a, b), [&#34;a&#34;, &#34;b&#34;])\n        &gt;&gt;&gt; df.agg(covar_samp(&#34;a&#34;, &#34;b&#34;).alias(&#39;c&#39;)).collect()\n        [Row(c=0.0)]\n    \n    crc32(col)\n        Calculates the cyclic redundancy check value  (CRC32) of a binary column and\n        returns the value as a bigint.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; spark.createDataFrame([(&#39;ABC&#39;,)], [&#39;a&#39;]).select(crc32(&#39;a&#39;).alias(&#39;crc32&#39;)).collect()\n        [Row(crc32=2743272264)]\n    \n    create_map(*cols)\n\n*** WARNING: skipped 74972 bytes of output ***\n\n        [Row(s=&#39;a.b&#39;)]\n        &gt;&gt;&gt; df.select(substring_index(df.s, &#39;.&#39;, -3).alias(&#39;s&#39;)).collect()\n        [Row(s=&#39;b.c.d&#39;)]\n    \n    sum(col)\n        Aggregate function: returns the sum of all values in the expression.\n        \n        .. versionadded:: 1.3\n    \n    sumDistinct(col)\n        Aggregate function: returns the sum of distinct values in the expression.\n        \n        .. versionadded:: 1.3\n    \n    tan(col)\n        .. versionadded:: 1.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            angle in radians\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            tangent of the given value, as if computed by `java.lang.Math.tan()`\n    \n    tanh(col)\n        .. versionadded:: 1.4.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            hyperbolic angle\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n            hyperbolic tangent of the given value\n            as if computed by `java.lang.Math.tanh()`\n    \n    timestamp_seconds(col)\n        .. versionadded:: 3.1.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql.functions import timestamp_seconds\n        &gt;&gt;&gt; spark.conf.set(&#34;spark.sql.session.timeZone&#34;, &#34;America/Los_Angeles&#34;)\n        &gt;&gt;&gt; time_df = spark.createDataFrame([(1230219000,)], [&#39;unix_time&#39;])\n        &gt;&gt;&gt; time_df.select(timestamp_seconds(time_df.unix_time).alias(&#39;ts&#39;)).show()\n        +-------------------+\n                 ts|\n        +-------------------+\n2008-12-25 07:30:00|\n        +-------------------+\n        &gt;&gt;&gt; spark.conf.unset(&#34;spark.sql.session.timeZone&#34;)\n    \n    toDegrees(col)\n        .. deprecated:: 2.1.0\n            Use :func:`degrees` instead.\n        \n        .. versionadded:: 1.4\n    \n    toRadians(col)\n        .. deprecated:: 2.1.0\n            Use :func:`radians` instead.\n        \n        .. versionadded:: 1.4\n    \n    to_csv(col, options={})\n        Converts a column containing a :class:`StructType` into a CSV string.\n        Throws an exception, in the case of an unsupported type.\n        \n        .. versionadded:: 3.0.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column containing a struct.\n        options: dict, optional\n            options to control converting. accepts the same options as the CSV datasource.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql import Row\n        &gt;&gt;&gt; data = [(1, Row(age=2, name=&#39;Alice&#39;))]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, (&#34;key&#34;, &#34;value&#34;))\n        &gt;&gt;&gt; df.select(to_csv(df.value).alias(&#34;csv&#34;)).collect()\n        [Row(csv=&#39;2,Alice&#39;)]\n    \n    to_date(col, format=None)\n        Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n        using the optionally specified format. Specify formats according to `datetime pattern`_.\n        By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n        is omitted. Equivalent to ``col.cast(&#34;date&#34;)``.\n        \n        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n        \n        .. versionadded:: 2.2.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;,)], [&#39;t&#39;])\n        &gt;&gt;&gt; df.select(to_date(df.t).alias(&#39;date&#39;)).collect()\n        [Row(date=datetime.date(1997, 2, 28))]\n        \n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;,)], [&#39;t&#39;])\n        &gt;&gt;&gt; df.select(to_date(df.t, &#39;yyyy-MM-dd HH:mm:ss&#39;).alias(&#39;date&#39;)).collect()\n        [Row(date=datetime.date(1997, 2, 28))]\n    \n    to_json(col, options={})\n        Converts a column containing a :class:`StructType`, :class:`ArrayType` or a :class:`MapType`\n        into a JSON string. Throws an exception, in the case of an unsupported type.\n        \n        .. versionadded:: 2.1.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column containing a struct, an array or a map.\n        options : dict, optional\n            options to control converting. accepts the same options as the JSON datasource.\n            Additionally the function supports the `pretty` option which enables\n            pretty JSON generation.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql import Row\n        &gt;&gt;&gt; from pyspark.sql.types import *\n        &gt;&gt;&gt; data = [(1, Row(age=2, name=&#39;Alice&#39;))]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, (&#34;key&#34;, &#34;value&#34;))\n        &gt;&gt;&gt; df.select(to_json(df.value).alias(&#34;json&#34;)).collect()\n        [Row(json=&#39;{&#34;age&#34;:2,&#34;name&#34;:&#34;Alice&#34;}&#39;)]\n        &gt;&gt;&gt; data = [(1, [Row(age=2, name=&#39;Alice&#39;), Row(age=3, name=&#39;Bob&#39;)])]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, (&#34;key&#34;, &#34;value&#34;))\n        &gt;&gt;&gt; df.select(to_json(df.value).alias(&#34;json&#34;)).collect()\n        [Row(json=&#39;[{&#34;age&#34;:2,&#34;name&#34;:&#34;Alice&#34;},{&#34;age&#34;:3,&#34;name&#34;:&#34;Bob&#34;}]&#39;)]\n        &gt;&gt;&gt; data = [(1, {&#34;name&#34;: &#34;Alice&#34;})]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, (&#34;key&#34;, &#34;value&#34;))\n        &gt;&gt;&gt; df.select(to_json(df.value).alias(&#34;json&#34;)).collect()\n        [Row(json=&#39;{&#34;name&#34;:&#34;Alice&#34;}&#39;)]\n        &gt;&gt;&gt; data = [(1, [{&#34;name&#34;: &#34;Alice&#34;}, {&#34;name&#34;: &#34;Bob&#34;}])]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, (&#34;key&#34;, &#34;value&#34;))\n        &gt;&gt;&gt; df.select(to_json(df.value).alias(&#34;json&#34;)).collect()\n        [Row(json=&#39;[{&#34;name&#34;:&#34;Alice&#34;},{&#34;name&#34;:&#34;Bob&#34;}]&#39;)]\n        &gt;&gt;&gt; data = [(1, [&#34;Alice&#34;, &#34;Bob&#34;])]\n        &gt;&gt;&gt; df = spark.createDataFrame(data, (&#34;key&#34;, &#34;value&#34;))\n        &gt;&gt;&gt; df.select(to_json(df.value).alias(&#34;json&#34;)).collect()\n        [Row(json=&#39;[&#34;Alice&#34;,&#34;Bob&#34;]&#39;)]\n    \n    to_timestamp(col, format=None)\n        Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.TimestampType`\n        using the optionally specified format. Specify formats according to `datetime pattern`_.\n        By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n        is omitted. Equivalent to ``col.cast(&#34;timestamp&#34;)``.\n        \n        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n        \n        .. versionadded:: 2.2.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;,)], [&#39;t&#39;])\n        &gt;&gt;&gt; df.select(to_timestamp(df.t).alias(&#39;dt&#39;)).collect()\n        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n        \n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;,)], [&#39;t&#39;])\n        &gt;&gt;&gt; df.select(to_timestamp(df.t, &#39;yyyy-MM-dd HH:mm:ss&#39;).alias(&#39;dt&#39;)).collect()\n        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n    \n    to_utc_timestamp(timestamp, tz)\n        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given\n        timezone, and renders that timestamp as a timestamp in UTC.\n        \n        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n        timezone-agnostic. So in Spark this function just shift the timestamp value from the given\n        timezone to UTC timezone.\n        \n        This function may return confusing result if the input is a string with timezone, e.g.\n        &#39;2018-03-13T06:18:23+00:00&#39;. The reason is that, Spark firstly cast the string to timestamp\n        according to the timezone in the string, and finally display the result by converting the\n        timestamp to string according to the session local timezone.\n        \n        .. versionadded:: 1.5.0\n        \n        Parameters\n        ----------\n        timestamp : :class:`~pyspark.sql.Column` or str\n            the column that contains timestamps\n        tz : :class:`~pyspark.sql.Column` or str\n            A string detailing the time zone ID that the input should be adjusted to. It should\n            be in the format of either region-based zone IDs or zone offsets. Region IDs must\n            have the form &#39;area/city&#39;, such as &#39;America/Los_Angeles&#39;. Zone offsets must be in\n            the format &#39;(+|-)HH:mm&#39;, for example &#39;-08:00&#39; or &#39;+01:00&#39;. Also &#39;UTC&#39; and &#39;Z&#39; are\n            upported as aliases of &#39;+00:00&#39;. Other short names are not recommended to use\n            because they can be ambiguous.\n        \n            .. versionchanged:: 2.4.0\n               `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28 10:30:00&#39;, &#39;JST&#39;)], [&#39;ts&#39;, &#39;tz&#39;])\n        &gt;&gt;&gt; df.select(to_utc_timestamp(df.ts, &#34;PST&#34;).alias(&#39;utc_time&#39;)).collect()\n        [Row(utc_time=datetime.datetime(1997, 2, 28, 18, 30))]\n        &gt;&gt;&gt; df.select(to_utc_timestamp(df.ts, df.tz).alias(&#39;utc_time&#39;)).collect()\n        [Row(utc_time=datetime.datetime(1997, 2, 28, 1, 30))]\n    \n    transform(col, f)\n        Returns an array of elements after applying a transformation to each element in the input array.\n        \n        .. versionadded:: 3.1.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        f : function\n            a function that is applied to each element of the input array.\n            Can take one of the following forms:\n        \n            - Unary ``(x: Column) -&gt; Column: ...``\n            - Binary ``(x: Column, i: Column) -&gt; Column...``, where the second argument is\n                a 0-based index of the element.\n        \n            and can use methods of :class:`~pyspark.sql.Column`, functions defined in\n            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n            Python ``UserDefinedFunctions`` are not supported\n            (`SPARK-27052 &lt;https://issues.apache.org/jira/browse/SPARK-27052&gt;`__).\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, [1, 2, 3, 4])], (&#34;key&#34;, &#34;values&#34;))\n        &gt;&gt;&gt; df.select(transform(&#34;values&#34;, lambda x: x * 2).alias(&#34;doubled&#34;)).show()\n        +------------+\n     doubled|\n        +------------+\n[2, 4, 6, 8]|\n        +------------+\n        \n        &gt;&gt;&gt; def alternate(x, i):\n        ...     return when(i % 2 == 0, x).otherwise(-x)\n        &gt;&gt;&gt; df.select(transform(&#34;values&#34;, alternate).alias(&#34;alternated&#34;)).show()\n        +--------------+\n    alternated|\n        +--------------+\n[1, -2, 3, -4]|\n        +--------------+\n    \n    transform_keys(col, f)\n        Applies a function to every key-value pair in a map and returns\n        a map with the results of those applications as the new keys for the pairs.\n        \n        .. versionadded:: 3.1.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        f : function\n            a binary function ``(k: Column, v: Column) -&gt; Column...``\n            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n            Python ``UserDefinedFunctions`` are not supported\n            (`SPARK-27052 &lt;https://issues.apache.org/jira/browse/SPARK-27052&gt;`__).\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, {&#34;foo&#34;: -2.0, &#34;bar&#34;: 2.0})], (&#34;id&#34;, &#34;data&#34;))\n        &gt;&gt;&gt; df.select(transform_keys(\n        ...     &#34;data&#34;, lambda k, _: upper(k)).alias(&#34;data_upper&#34;)\n        ... ).show(truncate=False)\n        +-------------------------+\ndata_upper               |\n        +-------------------------+\n{BAR -&gt; 2.0, FOO -&gt; -2.0}|\n        +-------------------------+\n    \n    transform_values(col, f)\n        Applies a function to every key-value pair in a map and returns\n        a map with the results of those applications as the new values for the pairs.\n        \n        .. versionadded:: 3.1.0\n        \n        Parameters\n        ----------\n        col : :class:`~pyspark.sql.Column` or str\n            name of column or expression\n        f : function\n            a binary function ``(k: Column, v: Column) -&gt; Column...``\n            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n            Python ``UserDefinedFunctions`` are not supported\n            (`SPARK-27052 &lt;https://issues.apache.org/jira/browse/SPARK-27052&gt;`__).\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, {&#34;IT&#34;: 10.0, &#34;SALES&#34;: 2.0, &#34;OPS&#34;: 24.0})], (&#34;id&#34;, &#34;data&#34;))\n        &gt;&gt;&gt; df.select(transform_values(\n        ...     &#34;data&#34;, lambda k, v: when(k.isin(&#34;IT&#34;, &#34;OPS&#34;), v + 10.0).otherwise(v)\n        ... ).alias(&#34;new_data&#34;)).show(truncate=False)\n        +---------------------------------------+\nnew_data                               |\n        +---------------------------------------+\n{OPS -&gt; 34.0, IT -&gt; 20.0, SALES -&gt; 2.0}|\n        +---------------------------------------+\n    \n    translate(srcCol, matching, replace)\n        A function translate any character in the `srcCol` by a character in `matching`.\n        The characters in `replace` is corresponding to the characters in `matching`.\n        The translate will happen when any character in the string matching with the character\n        in the `matching`.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; spark.createDataFrame([(&#39;translate&#39;,)], [&#39;a&#39;]).select(translate(&#39;a&#39;, &#34;rnlt&#34;, &#34;123&#34;) \\\n        ...     .alias(&#39;r&#39;)).collect()\n        [Row(r=&#39;1a2s3ae&#39;)]\n    \n    trim(col)\n        Trim the spaces from both ends for the specified string column.\n        \n        .. versionadded:: 1.5\n    \n    trunc(date, format)\n        Returns date truncated to the unit specified by the format.\n        \n        .. versionadded:: 1.5.0\n        \n        Parameters\n        ----------\n        date : :class:`~pyspark.sql.Column` or str\n        format : str\n            &#39;year&#39;, &#39;yyyy&#39;, &#39;yy&#39; to truncate by year,\n            or &#39;month&#39;, &#39;mon&#39;, &#39;mm&#39; to truncate by month\n            Other options are: &#39;week&#39;, &#39;quarter&#39;\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;1997-02-28&#39;,)], [&#39;d&#39;])\n        &gt;&gt;&gt; df.select(trunc(df.d, &#39;year&#39;).alias(&#39;year&#39;)).collect()\n        [Row(year=datetime.date(1997, 1, 1))]\n        &gt;&gt;&gt; df.select(trunc(df.d, &#39;mon&#39;).alias(&#39;month&#39;)).collect()\n        [Row(month=datetime.date(1997, 2, 1))]\n    \n    udf(f=None, returnType=StringType)\n        Creates a user defined function (UDF).\n        \n        .. versionadded:: 1.3.0\n        \n        Parameters\n        ----------\n        f : function\n            python function if used as a standalone function\n        returnType : :class:`pyspark.sql.types.DataType` or str\n            the return type of the user-defined function. The value can be either a\n            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n        \n        Examples\n        --------\n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; slen = udf(lambda s: len(s), IntegerType())\n        &gt;&gt;&gt; @udf\n        ... def to_upper(s):\n        ...     if s is not None:\n        ...         return s.upper()\n        ...\n        &gt;&gt;&gt; @udf(returnType=IntegerType())\n        ... def add_one(x):\n        ...     if x is not None:\n        ...         return x + 1\n        ...\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, &#34;John Doe&#34;, 21)], (&#34;id&#34;, &#34;name&#34;, &#34;age&#34;))\n        &gt;&gt;&gt; df.select(slen(&#34;name&#34;).alias(&#34;slen(name)&#34;), to_upper(&#34;name&#34;), add_one(&#34;age&#34;)).show()\n        +----------+--------------+------------+\nslen(name)|to_upper(name)|add_one(age)|\n        +----------+--------------+------------+\n         8|      JOHN DOE|          22|\n        +----------+--------------+------------+\n        \n        Notes\n        -----\n        The user-defined functions are considered deterministic by default. Due to\n        optimization, duplicate invocations may be eliminated or the function may even be invoked\n        more times than it is present in the query. If your function is not deterministic, call\n        `asNondeterministic` on the user defined function. E.g.:\n        \n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; import random\n        &gt;&gt;&gt; random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n        \n        The user-defined functions do not support conditional expressions or short circuiting\n        in boolean expressions and it ends up with being executed all internally. If the functions\n        can fail on special rows, the workaround is to incorporate the condition into the functions.\n        \n        The user-defined functions do not take keyword arguments on the calling side.\n    \n    unbase64(col)\n        Decodes a BASE64 encoded string column and returns it as a binary column.\n        \n        .. versionadded:: 1.5\n    \n    unhex(col)\n        Inverse of hex. Interprets each pair of characters as a hexadecimal number\n        and converts to the byte representation of number.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; spark.createDataFrame([(&#39;414243&#39;,)], [&#39;a&#39;]).select(unhex(&#39;a&#39;)).collect()\n        [Row(unhex(a)=bytearray(b&#39;ABC&#39;))]\n    \n    unix_timestamp(timestamp=None, format=&#39;yyyy-MM-dd HH:mm:ss&#39;)\n        Convert time string with given pattern (&#39;yyyy-MM-dd HH:mm:ss&#39;, by default)\n        to Unix time stamp (in seconds), using the default timezone and the default\n        locale, return null if fail.\n        \n        if `timestamp` is None, then it returns current timestamp.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; spark.conf.set(&#34;spark.sql.session.timeZone&#34;, &#34;America/Los_Angeles&#34;)\n        &gt;&gt;&gt; time_df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])\n        &gt;&gt;&gt; time_df.select(unix_timestamp(&#39;dt&#39;, &#39;yyyy-MM-dd&#39;).alias(&#39;unix_time&#39;)).collect()\n        [Row(unix_time=1428476400)]\n        &gt;&gt;&gt; spark.conf.unset(&#34;spark.sql.session.timeZone&#34;)\n    \n    upper(col)\n        Converts a string expression to upper case.\n        \n        .. versionadded:: 1.5\n    \n    var_pop(col)\n        Aggregate function: returns the population variance of the values in a group.\n        \n        .. versionadded:: 1.6\n    \n    var_samp(col)\n        Aggregate function: returns the unbiased sample variance of\n        the values in a group.\n        \n        .. versionadded:: 1.6\n    \n    variance(col)\n        Aggregate function: alias for var_samp\n        \n        .. versionadded:: 1.6\n    \n    weekofyear(col)\n        Extract the week number of a given date as integer.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])\n        &gt;&gt;&gt; df.select(weekofyear(df.dt).alias(&#39;week&#39;)).collect()\n        [Row(week=15)]\n    \n    when(condition, value)\n        Evaluates a list of conditions and returns one of multiple possible result expressions.\n        If :func:`pyspark.sql.Column.otherwise` is not invoked, None is returned for unmatched\n        conditions.\n        \n        .. versionadded:: 1.4.0\n        \n        Parameters\n        ----------\n        condition : :class:`~pyspark.sql.Column`\n            a boolean :class:`~pyspark.sql.Column` expression.\n        value :\n            a literal value, or a :class:`~pyspark.sql.Column` expression.\n        \n        &gt;&gt;&gt; df.select(when(df[&#39;age&#39;] == 2, 3).otherwise(4).alias(&#34;age&#34;)).collect()\n        [Row(age=3), Row(age=4)]\n        \n        &gt;&gt;&gt; df.select(when(df.age == 2, df.age + 1).alias(&#34;age&#34;)).collect()\n        [Row(age=3), Row(age=None)]\n    \n    window(timeColumn, windowDuration, slideDuration=None, startTime=None)\n        Bucketize rows into one or more time windows given a timestamp specifying column. Window\n        starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n        [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\n        the order of months are not supported.\n        \n        The time column must be of :class:`pyspark.sql.types.TimestampType`.\n        \n        Durations are provided as strings, e.g. &#39;1 second&#39;, &#39;1 day 12 hours&#39;, &#39;2 minutes&#39;. Valid\n        interval strings are &#39;week&#39;, &#39;day&#39;, &#39;hour&#39;, &#39;minute&#39;, &#39;second&#39;, &#39;millisecond&#39;, &#39;microsecond&#39;.\n        If the ``slideDuration`` is not provided, the windows will be tumbling windows.\n        \n        The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\n        window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\n        past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\n        \n        The output column will be a struct called &#39;window&#39; by default with the nested columns &#39;start&#39;\n        and &#39;end&#39;, where &#39;start&#39; and &#39;end&#39; will be of :class:`pyspark.sql.types.TimestampType`.\n        \n        .. versionadded:: 2.0.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#34;2016-03-11 09:00:07&#34;, 1)]).toDF(&#34;date&#34;, &#34;val&#34;)\n        &gt;&gt;&gt; w = df.groupBy(window(&#34;date&#34;, &#34;5 seconds&#34;)).agg(sum(&#34;val&#34;).alias(&#34;sum&#34;))\n        &gt;&gt;&gt; w.select(w.window.start.cast(&#34;string&#34;).alias(&#34;start&#34;),\n        ...          w.window.end.cast(&#34;string&#34;).alias(&#34;end&#34;), &#34;sum&#34;).collect()\n        [Row(start=&#39;2016-03-11 09:00:05&#39;, end=&#39;2016-03-11 09:00:10&#39;, sum=1)]\n    \n    xxhash64(*cols)\n        Calculates the hash code of given columns using the 64-bit variant of the xxHash algorithm,\n        and returns the result as a long column.\n        \n        .. versionadded:: 3.0.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; spark.createDataFrame([(&#39;ABC&#39;,)], [&#39;a&#39;]).select(xxhash64(&#39;a&#39;).alias(&#39;hash&#39;)).collect()\n        [Row(hash=4105715581806190027)]\n    \n    year(col)\n        Extract the year of a given date as integer.\n        \n        .. versionadded:: 1.5.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(&#39;2015-04-08&#39;,)], [&#39;dt&#39;])\n        &gt;&gt;&gt; df.select(year(&#39;dt&#39;).alias(&#39;year&#39;)).collect()\n        [Row(year=2015)]\n    \n    years(col)\n        Partition transform function: A transform for timestamps and dates\n        to partition data into years.\n        \n        .. versionadded:: 3.1.0\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df.writeTo(&#34;catalog.db.table&#34;).partitionedBy(  # doctest: +SKIP\n        ...     years(&#34;ts&#34;)\n        ... ).createOrReplace()\n        \n        Notes\n        -----\n        This function can be used only in combination with\n        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n        method of the `DataFrameWriterV2`.\n    \n    zip_with(left, right, f)\n        Merge two given arrays, element-wise, into a single array using a function.\n        If one array is shorter, nulls are appended at the end to match the length of the longer\n        array, before applying the function.\n        \n        .. versionadded:: 3.1.0\n        \n        Parameters\n        ----------\n        left : :class:`~pyspark.sql.Column` or str\n            name of the first column or expression\n        right : :class:`~pyspark.sql.Column` or str\n            name of the second column or expression\n        f : function\n            a binary function ``(x1: Column, x2: Column) -&gt; Column...``\n            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n            Python ``UserDefinedFunctions`` are not supported\n            (`SPARK-27052 &lt;https://issues.apache.org/jira/browse/SPARK-27052&gt;`__).\n        \n        Returns\n        -------\n        :class:`~pyspark.sql.Column`\n        \n        Examples\n        --------\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, [1, 3, 5, 8], [0, 2, 4, 6])], (&#34;id&#34;, &#34;xs&#34;, &#34;ys&#34;))\n        &gt;&gt;&gt; df.select(zip_with(&#34;xs&#34;, &#34;ys&#34;, lambda x, y: x ** y).alias(&#34;powers&#34;)).show(truncate=False)\n        +---------------------------+\npowers                     |\n        +---------------------------+\n[1.0, 9.0, 625.0, 262144.0]|\n        +---------------------------+\n        \n        &gt;&gt;&gt; df = spark.createDataFrame([(1, [&#34;foo&#34;, &#34;bar&#34;], [1, 2, 3])], (&#34;id&#34;, &#34;xs&#34;, &#34;ys&#34;))\n        &gt;&gt;&gt; df.select(zip_with(&#34;xs&#34;, &#34;ys&#34;, lambda x, y: concat_ws(&#34;_&#34;, x, y)).alias(&#34;xs_ys&#34;)).show()\n        +-----------------+\n            xs_ys|\n        +-----------------+\n[foo_1, bar_2, 3]|\n        +-----------------+\n\nFILE\n    /databricks/spark/python/pyspark/sql/functions.py\n\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.select(\n    current_timestamp().alias('current_timestamp'), \n    year(current_timestamp()).alias('year'),\n    month(current_timestamp()).alias('month'),\n    dayofmonth(current_timestamp()).alias('dayofmonth'),\n    hour(current_timestamp()).alias('hour'),\n    minute(current_timestamp()).alias('minute'),\n    second(current_timestamp()).alias('second')\n).show(truncate=False) #yyyy-MM-dd HH:mm:ss.SSS"],"metadata":{"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae029e83-85f0-40c7-98aa-6d1cfa1ab533"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------------------+----+-----+----------+----+------+------+\n|current_timestamp      |year|month|dayofmonth|hour|minute|second|\n+-----------------------+----+-----+----------+----+------+------+\n|2022-01-02 21:49:07.472|2022|1    |2         |21  |49    |7     |\n+-----------------------+----+-----+----------+----+------+------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------+----+-----+----------+----+------+------+\ncurrent_timestamp      |year|month|dayofmonth|hour|minute|second|\n+-----------------------+----+-----+----------+----+------+------+\n2022-01-02 21:49:07.472|2022|1    |2         |21  |49    |7     |\n+-----------------------+----+-----+----------+----+------+------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f87afae9-1850-43d6-a668-452e352a30a6"}},"outputs":[],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Pyspark 2","language":"python","name":"pyspark2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.12","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"15 Date and Time Extract Functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4062737143337759}},"nbformat":4,"nbformat_minor":0}
